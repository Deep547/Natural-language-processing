{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required packages for the given requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.classify.util import apply_features\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score,classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_char_repl(tweets): # function to replace the words in tweets with repeating characters\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1',tweets)\n",
    "def token(tweets):\n",
    "    tokenz = word_tokenize(tweets) #using the nltk package to tokenize the words in tweets\n",
    "    tokenized = ''\n",
    "    for j in tokenz:\n",
    "        tokenized += str(j +' ') # adding all the tokens with space to the tokenized variable\n",
    "    return tokenized.strip() # returns the tokenized variable after stripping the spaces\n",
    "def special_replace(tweets): # function to replace the special characters from tweet\n",
    "    tweets=re.sub(r'\\@\\S*','',tweets)\n",
    "    return tweets\n",
    "def non_alpha_num_replace(tweets): # function to remove non alphanumeric characters from tweets\n",
    "    tweets=re.sub(r'[^a-zA-Z0-9\\s]', '',tweets)\n",
    "    return tweets\n",
    "def num_replace(tweets): # function to remove numbers completely made of digits\n",
    "    tweets=re.sub(r'\\b[0-9]*\\b', '' , tweets)\n",
    "    return tweets\n",
    "def URL_replace(tweets): # function to remove the url's from the tweets\n",
    "    tweets=re.sub(r'\\bhttps?:\\/\\/\\S+[\\r\\n]*|(www\\.|reut\\.rs|bit\\.ly)\\S+|\\S+\\.(com|org|uk|info)\\b','',tweets)\n",
    "    tweets = re.sub(r'(?:(?:http?|https?):\\/\\/)?([-a-zA-Z0-9.]*\\.[a-z]*)\\b(?:\\/[-a-zA-Z0-9@:%_\\+.~?&//=]*)?', '',tweets)\n",
    "    return tweets\n",
    "def preprocessing(tweets): # This function calls all the above preprocessing functions and returns the processed tweets\n",
    "    tweets = tweets.lower()\n",
    "    tweets=URL_replace(tweets)\n",
    "    tweets = special_replace(tweets)\n",
    "    tweets=repeat_char_repl(tweets)\n",
    "    tweets=non_alpha_num_replace(tweets)\n",
    "    tweets=num_replace(tweets)\n",
    "    tweets=space_replace(tweets)\n",
    "    tweets=token(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words from the nltk package to remove all such words from the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') # access the stop words from nltk and cease analysis when you find words in tweet like is, a, uff, hmmm, uhh\n",
    "white_list_words=['no','we','have','not'] # list of the white listed words\n",
    "for ind in stop_words: # for loop to remove the whitelisted words from nltk based stop_words list, if it contains them\n",
    "    if ind in white_list_words:\n",
    "        stop_words.remove(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing the 1st classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "with open('twitter-training-data.txt',encoding=\"utf-8\") as file:\n",
    "    row=file.readlines()\n",
    "    for x in row:\n",
    "        x=x.strip() #stripping default whitespace characters from each of the lines of file.\n",
    "        term = x.split('\\t') #Splits the line on one or more tabs/spaces\n",
    "        list.append(term) #appends the values in term to the list\n",
    "list=pd.DataFrame(list) #converting the list into pandas dataframe\n",
    "list.columns=names=['tweet_id','sentiment','tweet_text'] #assigning headers to the specific columns in the list\n",
    "list.loc[:,\"tweet_text\"] = list.tweet_text.apply(lambda x: preprocessing(x))#using panda to access and preprocess the column tweet_text.\n",
    "list.loc[:,\"token_text\"] = list.tweet_text.apply(lambda x: [ind for ind in x.split() if ind not in (stop_words)]) #removing the stop words form the preprocessed twitter training data and assigning it to tweet_text\n",
    "list.loc[:,\"tweet_text\"] = list.token_text.apply(lambda x: \" \".join(x)) #joins the words under the header token_text with space and assigns it to dataframe header tweet_text\n",
    "#The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters\n",
    "#CountVectorizer from NLTK is used as vectizer to Convert a collection of text documents to a matrix of token counts\n",
    "#min_df=3 means When building the vocabulary ignore terms that have a document frequency strictly lower than 3.\n",
    "#ngram_range specifies the lower(here one grams) and upper boundary(here 2 grams) of the range of n-values for different n-grams to be extracted.\n",
    "#Logistic regression has been used as the classifier\n",
    "#random_state is the random number generator\n",
    "pipe = Pipeline([('vectorizer',CountVectorizer(min_df=3, ngram_range=(1,2))),('classifier',LogisticRegression(random_state=0) )])\n",
    "pipe.fit(list['tweet_text'].values, list['sentiment'].values) # fitting the logistic regression model with the list of tweet_text and sentiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing the second classifier and printing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development=[] # empty list\n",
    "with open('twitter-test3.txt',encoding=\"utf-8\") as file: # opening the twitter test dataset\n",
    "    row=file.readlines()\n",
    "    for x in row: # loop to fetch the lines in the file and to strip them of white space characters and further split the lines on tab spaces and append the values in a list.\n",
    "        x=x.strip()\n",
    "        term = x.split('\\t')\n",
    "        development.append(term)\n",
    "testing,list_t,identity=[],[],[] # three different lists defined.\n",
    "for i in range(len(development)): # for loop to loop through all the values in the list development\n",
    "    development[i][2]=preprocessing(development[i][2]) #looping through the list that benhaves like a two dimensional array, development, to access the text section of the tweets and to preprocess it.\n",
    "    testing.append(development[i][2]) #every preprocessed tweet text from the development list is appended into the testing list\n",
    "    list_t.append(development[i][1]) # every sentiment from the development list is appended into the list_t\n",
    "    identity.append(development[i][0]) # every tweet id from the development list is appended into the list identity\n",
    "development=pd.DataFrame(development) #converting the development list into pandas dataframe\n",
    "development.columns=names=['tweet_id','sentiment','tweet_text'] # assigning the column names to the dataframe development\n",
    "development.loc[:,\"token_text\"] = list.tweet_text.apply(lambda x: [ind for ind in x.split() if ind not in (stop_words)]) #removing the stop words form the tweet_text and assigning it to token_text\n",
    "prediction=pipe.predict(testing)#Predict using the linear regression model for the list testing that contains the preproceesed tweets text only with stop words removed , from file twitter-test3.txt\n",
    "print(prediction[300:350])\n",
    "accuracy_score(list_t,prediction)# Accuracy score predicts the accuracy of the correct predictions. Here list_t stores the true values of twitter sentiments and prediction holds the predicted values of twitter sentiments\n",
    "print(classification_report(list_t, prediction, target_names=set(list['sentiment'].values))) # classification report shows the main classification metrics.It takes the lists list_t,prediction as input and target names is set to the values of sentiments\n",
    "accuracy_score(prediction,list_t)\n",
    "pipe_line = Pipeline([('vectorizer',CountVectorizer(min_df=3, ngram_range=(1,2))),('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False)),(\"classifier\", LogisticRegression(random_state=0))])\n",
    "pipe_line.fit(list['tweet_text'].values, list['sentiment'].values)\n",
    "prediction=pipe_line.predict(testing)\n",
    "print(prediction[300:350])\n",
    "accuracy_score(prediction,list_t)\n",
    "print(classification_report( prediction,list_t, target_names=set(list['sentiment'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd Classifier: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_line = Pipeline([('vectorizer',CountVectorizer(min_df=3, ngram_range=(1,2))),('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False)),(\"classifier\", MultinomialNB())])\n",
    "pipe_line.fit(list['tweet_text'].values, list['sentiment'].values)\n",
    "prediction=pipe_line.predict(testing)\n",
    "print(prediction[300:350])\n",
    "accuracy_score(prediction,list_t)\n",
    "print(classification_report( prediction,list_t, target_names=set(list['sentiment'].values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
