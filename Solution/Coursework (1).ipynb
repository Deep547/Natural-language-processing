{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Corpus\n",
    "\n",
    "## Importing the required packages\n",
    "At first we import the required packages like nltk, download a couple of packages,import json for reading the json corpus, import the wordnet lemmatizer for lemmatizing words, import bigrams from nltk to identify the bigrams from corpus and finally import the word_tokenize package from the nltk.tokenize package to quickly and easily tokenize the words that we receive from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search function\n",
    "The purpose of this function is to search positive and negative words in the corpus using binary search.Binary search is an efficient algorithm for finding an item from an ordered list of items. It works by repeatedly dividing in half the portion of the list that could contain the item, until you've narrowed down the possible locations to just one.Same logic has been implemented in this function to search the positive and negative words in the corpus.Initially the begin variable is initialized to 0. Its value is set to one less than the length of the entire \"string\".The string argument in our case will be positive_word and negative_words.Then, a while loop is applied with condition that value of begin variable is less than finish.We identify the central location(of positive words corpus) for search by considering the integral average of begin and finish.Then the string at the central location is assigned to the variable focus.We compare the values(strings) of focus and desired_result.If length of focus string is greater than desired result & new value of the finish variable becomes center reduced by 1. However,if the length of focus variable is lesser than desired result then, begin variable becomes equivalent to centre+1. If none of the above condition holds i.e both variables are of same size then focus is returned. Once the condition begin <= finish no more holds true the while loop quits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(string,desired_result):  \n",
    "    begin = 0 \n",
    "    finish = len(string) - 1  \n",
    "    while begin <= finish:   \n",
    "        centre = int((begin + finish) / 2) \n",
    "        focus = string[centre]  \n",
    "        if focus > desired_result:\n",
    "            finish = centre - 1  \n",
    "        elif focus < desired_result: \n",
    "            begin = centre + 1\n",
    "        else: \n",
    "            return focus\n",
    "    return -1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## find_word function\n",
    "The purpose of this function is to determine the number of positive and negative words in the corpus provided.Then the variable negative which implies number of negative words is initialized to value 0 and variable positive which implies the number of positive words is initialized to value 0.For loop to use to loop through the lemmatized tokens stored in the token list.Then,search() function is called and positive_word and ind(which holds each line of the token)  variables are sent as arguments.The output returned by the search function is stored in the variable i.Moreover an if loop is used to check if the value assigned to i variable is not -1 then,count of the variable positive is incremented by 1.However when i has value -1 assigned to it.Search function is called and positive_word and ind variables are sent as arguments.The output returned is stored in the variable i. Further, if the value returned is not -1 then the negative variable is incremented by 1.Finally the function returns the value of the positive and the negative variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(token): \n",
    "    negative = 0  \n",
    "    positive = 0 \n",
    "    for ind in token: \n",
    "        i = search(positive_word,ind)\n",
    "        if (i != -1): \n",
    "            positive += 1\n",
    "        else: \n",
    "            i = search(negative_word,ind)\n",
    "            if (i != -1):   \n",
    "                negative += 1\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The wordnet lemmatizer is being used over here. Then a set of empty lists are defined and a set of variables are initialized with zero value.Further the json file signal-news1.jsonl is opened as file and then by using a for loop we loop through each and every line of the file reading specially the content section. Then as a part of preproceesing, the entire content is converted into lower case. Further, the regex expression is applied to remove the various types of url's, non-alphanumeric characters except space, remove words with 3 characters or fewer, removing strings fully made up of digits. Then we tokenise the preprocessed output received. Further, all the tokens in a are lemmatized using a for loop and the output is assigned to the variable token.The preprocessed output obtained by filtering the contents of json file after applying the regular expression is stored into the list news_variety. Then an if condition checks if counter is less than 16000 and if the condition is true, it lemmatizes all the tokens in a assigning these tokens to list training. When the number of lines becomes greater than 16000,the lemmatized tokens obtained from a are assigned to the list testing.Finally, counter is incremented at the end of for loop.\n",
    "After that the total number of token and the total vocabulary size is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer() \n",
    "news_variety, training, testing, token, negative_word, positive_word = [], [], [], [], [], [] \n",
    "news_positive, news_negative, counter = 0, 0, 0  \n",
    "with open(\"signal-news1.jsonl\") as file: \n",
    "    for line in file:  \n",
    "        lines = json.loads(line)\n",
    "        content = (lines['content']) \n",
    "        content = content.lower() \n",
    "        a = re.sub(r'https?:\\/\\/\\S+[\\r\\n]*|(www\\.|reut\\.rs|bit\\.ly)\\S+|\\S+\\.(com|org|uk|info)', '', content,flags=re.MULTILINE) \n",
    "        a = re.sub(r'(?:(?:http?|https?):\\/\\/)?([-a-zA-Z0-9.]*\\.[a-z]*)\\b(?:\\/[-a-zA-Z0-9@:%_\\+.~?&//=]*)?', '',content)\n",
    "        b = re.sub(r'[^a-zA-Z0-9\\s]', '', a)  \n",
    "        c = re.sub(r'\\b\\w{1,3}\\b', '', b)  \n",
    "        d = re.sub(r'\\b[0-9]+\\b', '', c)   \n",
    "        a = word_tokenize(d) \n",
    "        token += (lemma.lemmatize(e) for e in a) \n",
    "        news_variety.append(d)  \n",
    "        if counter < 16000: \n",
    "            training += (lemma.lemmatize(e) for e in a)\n",
    "        else:  \n",
    "            testing += (lemma.lemmatize(e) for e in a)\n",
    "        counter += 1\n",
    "\n",
    "print(\"The Total number of tokens is : \" + str(len(token)))  \n",
    "print(\"The Total vocabulary size is : \" + str(len(set(token)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "In this part I have made use of a dictionary to access and print the 25 bigrams that have the highest frequency.The reason that I have used dictionary instead of list is because its much more quicker to fetch records from dictionary by specifying the element's hash keys .So, at first empty dictionary is declared here in order to store the bigrams into it and to access them quickly. Then, a for loop is used to loop through bigrams of token variable.Intially, using the else part the first bigram is inserted into the dictionary dict and then the for loop uses the if loop to update the members inside the dictionary dict.Further, we access the dictionary of bigrams ordered by highest frequency and then list the top 25 bigrams based on the number of occurrences on the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {} \n",
    "for twogram in bigrams(token): \n",
    "    if twogram in dict:  \n",
    "        dict[twogram] += 1\n",
    "    else:\n",
    "        dict[twogram] = 1\n",
    "#print(dict)\n",
    "\n",
    "frequent_bigrams = sorted(dict, key=dict.get,reverse=True)  \n",
    "print(\"The most frequent bigrams are:\\n\",frequent_bigrams[:25]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the total number of postive and negative words\n",
    "In this section I have printed the total number of positive and ngative words. In order to do that,we first begin with opening the file negative-words.txt as a to read the contents by specifying the encoding. Then, we read the lines from the file a, after the line 35 into the variable file(as lines from 1-35 can be ignored). Further, a for loop is used to loop through each line in the file. Then, the list negative_word here is appended after stripping each of the lines from the default whitespace characters. Same set of steps are repeated for the file positive-words.txt and the positive words are appended into the list positive_word. Then the function find_word() is called by passing token as an argument and the output(two values) returned is assigned to the variables pos i.e positive and negative. Finally, the total number of positive and negative words are printed by printing the values of the variables pos and neg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"negative-words.txt\",encoding = \"ISO-8859-1\") as a:\n",
    "    file = a.readlines()[35:] \n",
    "    for lines in file:  \n",
    "        negative_word.append(lines.strip())\n",
    "\n",
    "with open(\"positive-words.txt\",encoding = \"ISO-8859-1\") as a:\n",
    "    file = a.readlines()[35:] \n",
    "    for lines in file:\n",
    "        positive_word.append(lines.strip()) \n",
    "\n",
    "pos,neg=find_word(token)\n",
    "print (\"The count of the positive words is : \"+ str(pos)) \n",
    "print (\"The count of the negative words is : \"+str(neg)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the total number of positive and negative news stories\n",
    "In this section I have tried to compute the number of news stories with more positive than negative words, as well as the number of news stories with more negative than positive words. In order to do the same at first I have used a for loop to loop through each and every item i.e news in the news_variety list. Further, I split every item into strings and assign it to the variable token. Then, the find_word function is called by passing token variable as argument and output returned is assigned to the variables pos and neg. Then an if loop is ran to check, if pos variable is greater than neg. If the conditio is satisfied then the news_positive variable is incremented by 1,which means news is considered more positive. However, in other situation news_negative variable is incremented by 1,which means news is considered more negative. Finally, after an exhaustive search of each and every member in news_variety, the total number of positive and total number of negative news stories is printed by printing the news_positive and news_negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in news_variety:\n",
    "    token=news.split() \n",
    "    pos,neg=find_word(token) \n",
    "    if pos>neg: \n",
    "        news_positive+=1\n",
    "    else: \n",
    "        news_negative+=1\n",
    "print(\"The number of Positive news stories is : \"+str(news_positive)) #prints the total number of positive news stories\n",
    "print(\"The number of Negative news stories is : \"+str(news_negative)) #prints the total number of negative news stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models for bigrams\n",
    "In order to build this model we had seen that in the preprocessing section itself that we had sen that if counter is less than 16000 and if the condition is true, it lemmatizes all the tokens in a assigning these tokens to list training. When the number of lines becomes greater than 16000,the lemmatized tokens obtained from a are assigned to the list testing. In this section we have a training pair and a testing pair for building the language model that produce a sentence beginning with the word “They,”  of 10 words by appending the most likely next word each time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pair = {} #An empty dictionary training_pair is created\n",
    "for twogram in bigrams(training): #for loop to loop through bigrams of training variable.Using the else part the first bigram is inserted into the dictionary testing_pair\n",
    "    if twogram in training_pair:\n",
    "        training_pair[twogram] += 1 #the for loop uses the if loop to update the members inside the dictionary testing_pair.\n",
    "    else:\n",
    "        training_pair[twogram] = 1\n",
    "training_frequent_bigrams = sorted(training_pair, key=training_pair.get, reverse=True) # accessing the dictionary(training_pair) of bigrams ordered by highest frequency\n",
    "#print(training_frequent_bigrams)\n",
    "sentence_they = [\"they\"] # variable declared\n",
    "for i in range(0, 9): # for loop loops through values 0 to 9\n",
    "    for a in training_frequent_bigrams: # nested for loop to loop through the members of training_frequent_bigrams dictionary\n",
    "        if a[0] == sentence_they[i]: # checks continuously if the first word of the sentence is they\n",
    "            break\n",
    "    sentence_they.append(a[1]) # keeps on appending the sentence when the if loop breaks.\n",
    "print('The sentence predicted to start with \"They\" of length 10 words is:\\n',\" \".join(sentence_they))\n",
    "\n",
    "testing_pair = {}  # An empty dictionary testing_pair is created\n",
    "for twogram in bigrams(\n",
    "        testing):  # for loop to loop through bigrams of testing variable.Using the else part the first bigram is inserted into the dictionary testing_pair\n",
    "    if twogram in testing_pair:\n",
    "        testing_pair[\n",
    "            twogram] += 1  # the for loop uses the if loop to update the members inside the dictionary testing_pair.\n",
    "    else:\n",
    "        testing_pair[twogram] = 1\n",
    "        testing_frequent_bigrams = sorted(testing_pair, key=testing_pair.get,reverse=True)  # accessing the dictionary(testing_pair) of bigrams ordered by highest frequency\n",
    "#print(testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
